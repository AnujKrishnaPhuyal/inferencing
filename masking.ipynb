{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099412a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight-based masking: Higher weights → More tokens masked\n",
    "# Using CodeBERT tokenizer for [MASK] token\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def mask_line_tokens_weighted_codebert(line_text, mask_percentage):\n",
    "    \"\"\"\n",
    "    Mask a percentage of tokens in a line using CodeBERT tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        line_text: The line of code to mask\n",
    "        mask_percentage: Percentage of tokens to mask (based on weight)\n",
    "    \n",
    "    Returns:\n",
    "        masked_text: Line with masked tokens\n",
    "        mask_positions: List of token positions that were masked\n",
    "        original_tokens: List of original tokens that were masked\n",
    "    \"\"\"\n",
    "    if not line_text.strip():\n",
    "        return line_text, [], []\n",
    "    \n",
    "    # Tokenize the line using CodeBERT tokenizer\n",
    "    line_tokens = codebert_tokenizer(line_text, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    n_line_tokens = len(line_tokens)\n",
    "    \n",
    "    if n_line_tokens == 0:\n",
    "        return line_text, [], []\n",
    "    \n",
    "    # Calculate number of tokens to mask based on percentage\n",
    "    num_to_mask = max(1, int(n_line_tokens * mask_percentage))\n",
    "    num_to_mask = min(num_to_mask, n_line_tokens)  # Don't exceed total tokens\n",
    "    \n",
    "    # Randomly select positions to mask\n",
    "    mask_positions = random.sample(range(n_line_tokens), num_to_mask)\n",
    "    mask_positions.sort()\n",
    "    \n",
    "    # Store original tokens that will be masked\n",
    "    original_tokens = []\n",
    "    masked_token_ids = line_tokens.clone()\n",
    "    \n",
    "    # Use CodeBERT's [MASK] token\n",
    "    mask_token_id = codebert_tokenizer.mask_token_id\n",
    "    \n",
    "    for pos in mask_positions:\n",
    "        original_tokens.append({\n",
    "            'position': pos,\n",
    "            'token_id': line_tokens[pos].item(),\n",
    "            'token_text': codebert_tokenizer.decode([line_tokens[pos]])\n",
    "        })\n",
    "        masked_token_ids[pos] = mask_token_id\n",
    "    \n",
    "    # Decode the masked line using CodeBERT tokenizer\n",
    "    masked_text = codebert_tokenizer.decode(masked_token_ids, skip_special_tokens=False)\n",
    "    \n",
    "    return masked_text, mask_positions, original_tokens\n",
    "\n",
    "# Weight-based masking configuration\n",
    "random.seed(42)  # For reproducibility\n",
    "\n",
    "# Base masking percentage\n",
    "BASE_MASK_PCT = 0.15  # 15% base masking\n",
    "\n",
    "# Apply weights to determine actual masking\n",
    "# Higher weight → Multiply by factor > 1 (more masking)\n",
    "# Lower weight → Multiply by factor < 1 (less masking)\n",
    "\n",
    "# Calculate masking percentage for each line based on its weight\n",
    "# We'll scale the base 15% by the relative weight\n",
    "line_stats['mask_percentage'] = line_stats['weight_normalized'].apply(\n",
    "    lambda w: BASE_MASK_PCT * (w / line_stats['weight_normalized'].mean())\n",
    ")\n",
    "\n",
    "# Clip to reasonable range (min 5%, max 30%)\n",
    "line_stats['mask_percentage'] = line_stats['mask_percentage'].clip(0.05, 0.30)\n",
    "\n",
    "masked_results = []\n",
    "\n",
    "for idx, row in line_stats.iterrows():\n",
    "    line_text = row['line_content']\n",
    "    mask_pct = row['mask_percentage']\n",
    "    \n",
    "    masked_text, mask_pos, orig_tokens = mask_line_tokens_weighted_codebert(line_text, mask_pct)\n",
    "    \n",
    "    masked_results.append({\n",
    "        'line': row['line'],\n",
    "        'original_line': line_text,\n",
    "        'masked_line': masked_text,\n",
    "        'num_tokens': row['num_tokens'],\n",
    "        'num_masked': len(mask_pos),\n",
    "        'mask_percentage_actual': (len(mask_pos) / row['num_tokens'] * 100) if row['num_tokens'] > 0 else 0,\n",
    "        'mask_percentage_target': mask_pct * 100,\n",
    "        'masked_positions': mask_pos,\n",
    "        'masked_tokens': orig_tokens,\n",
    "        'perplexity': row['perplexity'],\n",
    "        'weight': row['weight_normalized']\n",
    "    })\n",
    "\n",
    "masked_df = pd.DataFrame(masked_results)\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"WEIGHT-BASED MASKING WITH CODEBERT (15% Base Rate × Weight Factor)\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"\\nTotal lines: {len(masked_df)}\")\n",
    "print(f\"Base masking: {BASE_MASK_PCT*100:.1f}% of tokens\")\n",
    "print(f\"Weight adjustment: Lines scaled by relative weight (higher weight → more masking)\")\n",
    "print(f\"Actual range: {masked_df['mask_percentage_target'].min():.1f}% to {masked_df['mask_percentage_target'].max():.1f}%\")\n",
    "print(f\"Tokenizer: CodeBERT (microsoft/codebert-base)\")\n",
    "print(f\"Mask token: {codebert_tokenizer.mask_token}\\n\")\n",
    "\n",
    "# Display summary sorted by weight (descending)\n",
    "display_cols = ['line', 'weight', 'perplexity', 'num_tokens', 'num_masked', 'mask_percentage_target', 'mask_percentage_actual']\n",
    "summary_sorted = masked_df[display_cols].sort_values('weight', ascending=False)\n",
    "\n",
    "print(\"\\nSummary (sorted by weight - highest first):\")\n",
    "print(summary_sorted.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"Sample Lines (Top 5 highest weighted = most masked):\")\n",
    "print(\"=\" * 120)\n",
    "top_weighted = masked_df.nlargest(5, 'weight')\n",
    "for i, row in top_weighted.iterrows():\n",
    "    print(f\"\\nLine {row['line']} (Weight: {row['weight']:.4f}, Perplexity: {row['perplexity']:.2f}):\")\n",
    "    print(f\"  Original: {row['original_line']}\")\n",
    "    print(f\"  Masked:   {row['masked_line']}\")\n",
    "    print(f\"  Masked {row['num_masked']}/{row['num_tokens']} tokens ({row['mask_percentage_actual']:.1f}%)\")\n",
    "\n",
    "masked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de02b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate perturbations for all masked lines\n",
    "import time\n",
    "\n",
    "# Configuration for perturbation\n",
    "NUM_PERTURBATIONS = 2\n",
    "TOP_P = 0.5  # Nucleus sampling threshold\n",
    "TEMPERATURE = 0.9  # Sampling temperature\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(f\"GENERATING PERTURBATIONS USING CODEBERT WITH NUCLEUS SAMPLING\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"Number of perturbations per line: {NUM_PERTURBATIONS}\")\n",
    "print(f\"Sampling method: Nucleus sampling (top-p={TOP_P}, temperature={TEMPERATURE})\")\n",
    "print(f\"Total masked lines: {len(masked_df)}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "perturbation_results = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, row in masked_df.iterrows():\n",
    "    masked_line = row['masked_line']\n",
    "    \n",
    "    # Skip lines with no masks\n",
    "    if '<mask>' not in masked_line.lower() and '[MASK]' not in masked_line:\n",
    "        perturbations = [row['original_line']] * NUM_PERTURBATIONS\n",
    "    else:\n",
    "        # Generate perturbations\n",
    "        perturbations = generate_perturbations_for_masked_line(\n",
    "            masked_line, \n",
    "            num_perturbations=NUM_PERTURBATIONS,\n",
    "            top_p=TOP_P,\n",
    "            temperature=TEMPERATURE\n",
    "        )\n",
    "    \n",
    "    perturbation_results.append({\n",
    "        'line': row['line'],\n",
    "        'original_line': row['original_line'],\n",
    "        'masked_line': masked_line,\n",
    "        'perturbation_1': perturbations[0] if len(perturbations) > 0 else '',\n",
    "        'perturbation_2': perturbations[1] if len(perturbations) > 1 else '',\n",
    "        'num_masked': row['num_masked'],\n",
    "        'perplexity': row['perplexity'],\n",
    "        'weight': row['weight']\n",
    "    })\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(masked_df)} lines...\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "perturbation_df = pd.DataFrame(perturbation_results)\n",
    "\n",
    "print(f\"\\n✓ Perturbation generation complete!\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Average time per line: {(end_time - start_time) / len(masked_df):.3f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"SAMPLE PERTURBATIONS (Top 5 highest weighted lines):\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "top_samples = perturbation_df.nlargest(5, 'weight')\n",
    "for i, row in top_samples.iterrows():\n",
    "    print(f\"\\nLine {row['line']} (Weight: {row['weight']:.4f}, Perplexity: {row['perplexity']:.2f}):\")\n",
    "    print(f\"  Original:        {row['original_line']}\")\n",
    "    print(f\"  Masked:          {row['masked_line']}\")\n",
    "    print(f\"  Perturbation 1:  {row['perturbation_1']}\")\n",
    "    print(f\"  Perturbation 2:  {row['perturbation_2']}\")\n",
    "\n",
    "perturbation_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
